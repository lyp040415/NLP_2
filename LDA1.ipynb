{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import jieba\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(corpus_dir, K, token_mode='word', sample_size=1000):\n",
    "    corpus = []\n",
    "    labels = []\n",
    "    txt_files = glob.glob(os.path.join(corpus_dir, \"*.txt\"))\n",
    "    for file in txt_files:\n",
    "        with open(file, 'r', encoding='gb18030', errors='ignore') as f:\n",
    "            text = f.read().strip()\n",
    "            if token_mode == 'word':\n",
    "                tokens = list(jieba.cut(text))\n",
    "            elif token_mode == 'char':\n",
    "                tokens = list(text)\n",
    "            else:\n",
    "                raise ValueError(\"token_mode must be 'word' or 'char'\")\n",
    "            # Split into paragraphs of K tokens\n",
    "            num_paragraphs = len(tokens) // K\n",
    "            for i in range(num_paragraphs):\n",
    "                paragraph = tokens[i*K : (i+1)*K]\n",
    "                corpus.append(' '.join(paragraph))\n",
    "                labels.append(os.path.basename(file).replace('.txt', ''))\n",
    "    # Convert to numpy arrays\n",
    "    corpus = np.array(corpus)\n",
    "    labels = np.array(labels)\n",
    "    # Random sample\n",
    "    if len(corpus) < sample_size:\n",
    "        raise ValueError(f\"Not enough paragraphs. Only {len(corpus)} available.\")\n",
    "    indices = np.random.choice(len(corpus), size=sample_size, replace=False)\n",
    "    return corpus[indices], labels[indices]\n",
    "\n",
    "def evaluate(corpus, labels, T, token_mode='word', n_splits=10, test_size=100):\n",
    "    cv = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=42)\n",
    "    accuracies = []\n",
    "    for train_idx, test_idx in cv.split(corpus):\n",
    "        X_train, X_test = corpus[train_idx], corpus[test_idx]\n",
    "        y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "        \n",
    "        # Vectorization\n",
    "        vectorizer = CountVectorizer()\n",
    "        X_train_counts = vectorizer.fit_transform(X_train)\n",
    "        X_test_counts = vectorizer.transform(X_test)\n",
    "        \n",
    "        # LDA\n",
    "        lda = LatentDirichletAllocation(n_components=T, random_state=42)\n",
    "        X_train_lda = lda.fit_transform(X_train_counts)\n",
    "        X_test_lda = lda.transform(X_test_counts)\n",
    "        \n",
    "        # Classification\n",
    "        clf = SVC(kernel='linear', random_state=42)\n",
    "        clf.fit(X_train_lda, y_train)\n",
    "        y_pred = clf.predict(X_test_lda)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    return np.mean(accuracies)\n",
    "\n",
    "def main():\n",
    "    corpus_dir = \"D:/课程/大四/大四下/自然语言处理/第二次作业/jyxstxtqj_downcc.com/\"\n",
    "    results = []\n",
    "    \n",
    "    # 实验参数配置（示例用较小参数组合）\n",
    "    K_values = [20, 100, 500, 1000, 3000]  # 完整实验需包含所有指定K值\n",
    "    T_values = [5, 10, 20, 50, 100]\n",
    "    token_modes = ['word', 'char']\n",
    "    \n",
    "    for K in K_values:\n",
    "        for token_mode in token_modes:\n",
    "            try:\n",
    "                print(f\"\\nProcessing K={K}, mode={token_mode}...\")\n",
    "                corpus, labels = load_data(corpus_dir, K=K, token_mode=token_mode)\n",
    "                for T in T_values:\n",
    "                    acc = evaluate(corpus, labels, T=T, token_mode=token_mode)\n",
    "                    results.append({'K': K, 'Mode': token_mode, 'T': T, 'Accuracy': acc})\n",
    "                    print(f\"K={K}, Mode={token_mode}, T={T} => Accuracy: {acc:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error with K={K}, mode={token_mode}: {str(e)}\")\n",
    "    \n",
    "    # 结果展示与分析\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\nResults Summary:\")\n",
    "    print(df)\n",
    "    \n",
    "    # 结果讨论\n",
    "    print(\"\\nAnalysis:\")\n",
    "    print(\"1. 主题数量T的影响：随着T增加，准确率可能先升后降，最佳T值需平衡信息量与噪声。\")\n",
    "    print(\"2. 分词vs分字：分词通常携带更多语义信息，但分字对未登录词更鲁棒。\")\n",
    "    print(\"3. 段落长度K：短文本（K小）信息不足，长文本（K大）可能包含多主题，需适中长度。\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing K=20, mode=word...\n",
      "K=20, Mode=word, T=5 => Accuracy: 0.1420\n",
      "K=20, Mode=word, T=10 => Accuracy: 0.1400\n",
      "K=20, Mode=word, T=20 => Accuracy: 0.1400\n",
      "K=20, Mode=word, T=50 => Accuracy: 0.1620\n",
      "K=20, Mode=word, T=100 => Accuracy: 0.1760\n",
      "\n",
      "Processing K=20, mode=char...\n",
      "Error with K=20, mode=char: empty vocabulary; perhaps the documents only contain stop words\n",
      "\n",
      "Processing K=100, mode=word...\n",
      "K=100, Mode=word, T=5 => Accuracy: 0.1700\n",
      "K=100, Mode=word, T=10 => Accuracy: 0.1730\n",
      "K=100, Mode=word, T=20 => Accuracy: 0.1860\n",
      "K=100, Mode=word, T=50 => Accuracy: 0.2060\n",
      "K=100, Mode=word, T=100 => Accuracy: 0.2250\n",
      "\n",
      "Processing K=100, mode=char...\n",
      "Error with K=100, mode=char: empty vocabulary; perhaps the documents only contain stop words\n",
      "\n",
      "Processing K=500, mode=word...\n",
      "K=500, Mode=word, T=5 => Accuracy: 0.2660\n",
      "K=500, Mode=word, T=10 => Accuracy: 0.3080\n",
      "K=500, Mode=word, T=20 => Accuracy: 0.3500\n",
      "K=500, Mode=word, T=50 => Accuracy: 0.3640\n",
      "K=500, Mode=word, T=100 => Accuracy: 0.3810\n",
      "\n",
      "Processing K=500, mode=char...\n",
      "Error with K=500, mode=char: empty vocabulary; perhaps the documents only contain stop words\n",
      "\n",
      "Processing K=1000, mode=word...\n",
      "K=1000, Mode=word, T=5 => Accuracy: 0.3530\n",
      "K=1000, Mode=word, T=10 => Accuracy: 0.3790\n",
      "K=1000, Mode=word, T=20 => Accuracy: 0.4550\n",
      "K=1000, Mode=word, T=50 => Accuracy: 0.5010\n",
      "K=1000, Mode=word, T=100 => Accuracy: 0.5490\n",
      "\n",
      "Processing K=1000, mode=char...\n",
      "Error with K=1000, mode=char: empty vocabulary; perhaps the documents only contain stop words\n",
      "\n",
      "Processing K=3000, mode=word...\n",
      "K=3000, Mode=word, T=5 => Accuracy: 0.5570\n",
      "K=3000, Mode=word, T=10 => Accuracy: 0.6720\n",
      "K=3000, Mode=word, T=20 => Accuracy: 0.7420\n",
      "K=3000, Mode=word, T=50 => Accuracy: 0.7970\n",
      "K=3000, Mode=word, T=100 => Accuracy: 0.7780\n",
      "\n",
      "Processing K=3000, mode=char...\n",
      "Error with K=3000, mode=char: empty vocabulary; perhaps the documents only contain stop words\n",
      "\n",
      "Results Summary:\n",
      "       K  Mode    T  Accuracy\n",
      "0     20  word    5     0.142\n",
      "1     20  word   10     0.140\n",
      "2     20  word   20     0.140\n",
      "3     20  word   50     0.162\n",
      "4     20  word  100     0.176\n",
      "5    100  word    5     0.170\n",
      "6    100  word   10     0.173\n",
      "7    100  word   20     0.186\n",
      "8    100  word   50     0.206\n",
      "9    100  word  100     0.225\n",
      "10   500  word    5     0.266\n",
      "11   500  word   10     0.308\n",
      "12   500  word   20     0.350\n",
      "13   500  word   50     0.364\n",
      "14   500  word  100     0.381\n",
      "15  1000  word    5     0.353\n",
      "16  1000  word   10     0.379\n",
      "17  1000  word   20     0.455\n",
      "18  1000  word   50     0.501\n",
      "19  1000  word  100     0.549\n",
      "20  3000  word    5     0.557\n",
      "21  3000  word   10     0.672\n",
      "22  3000  word   20     0.742\n",
      "23  3000  word   50     0.797\n",
      "24  3000  word  100     0.778\n",
      "\n",
      "Analysis:\n",
      "1. 主题数量T的影响：随着T增加，准确率可能先升后降，最佳T值需平衡信息量与噪声。\n",
      "2. 分词vs分字：分词通常携带更多语义信息，但分字对未登录词更鲁棒。\n",
      "3. 段落长度K：短文本（K小）信息不足，长文本（K大）可能包含多主题，需适中长度。\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dig_life",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
